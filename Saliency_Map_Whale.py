# -*- coding: utf-8 -*-
"""dsl_part_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nls4KBTD1tGCRe3aUX0FYS1kuelHk6JR

[Our previous attempt](https://www.kaggle.com/code/lucasgvazquez/road-to-the-top-part-1-a-dive-to-the-bottom) was a complete failure. We spent too much time trying to simplify the task but the result was much worse than a uniform predictor.

Lesson learned: Setup a basic model and aim for a first submission ASAP. It might sound a bit against "best practices", but when starting a new task, you should not pay much attention to the data. Deep exploratory data analysis (EDA) can give valuable insights, but in the beginning we just need to know the bare minimum to get a model training.

This is the notebook that should have actually been the first in the series: Let's train a tiny model, with very small images, using the entire dataset.

If you find this work useful, please remember to click the upvote button at the top. That will help others finding this work, and it will also help us ([Lucas](https://twitter.com/lucasgvazquez) and [Francesco](https://twitter.com/Fra_Pochetti)) to get some recognition and stay motivated to keep producing these :)

## Setup

Downloading libraries and imports same as before. We'll be hiding those cells.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install kaggle

# Commented out IPython magic to ensure Python compatibility.
try: import fastkaggle
except ModuleNotFoundError:
#     %pip install -Uq fastkaggle

from fastkaggle import *

# Commented out IPython magic to ensure Python compatibility.
try: import timm
except ModuleNotFoundError:
#     %pip install "timm>=0.6.2.dev0"

import timm

print(timm.__version__)

# Commented out IPython magic to ensure Python compatibility.
# %pip install fastai --upgrade
from fastai.vision.all import *

# Commented out IPython magic to ensure Python compatibility.
# %pip install opendatasets
from torchvision import models
import torch.nn as nn

import opendatasets as od
od.download("https://www.kaggle.com/competitions/noaa-right-whale-recognition/")

comp = 'noaa-right-whale-recognition'
data_dir = setup_comp(comp, install='fastai "timm>=0.6.2.dev0"')
imgs_dir = untar_dir(data_dir/'imgs.zip', Path('imgs'))

"""Once again, let's keep the goal in mind: We want to do the **bare-minimum** to have an initial model training. It's surprisingly hard to define what the bare minimum is. Ideally, we just want to ingest the dataset as it's, randomly split the data, define basic transforms (resize + augmentations), and train the model.

Unfortunately, most tasks will require some extra steps to get there. For sake of clarity, we'll be tagging those as **Extra step**. In the comments, you can give your opinion if these extra steps were really required or if we could have done something differently.

**Extra step:** What is the average size of images in the dataset? Having images that are too big will bottleneck the CPU (when resizing) and training will be very slow. Let's check the average size of images and resize them if necessary.
"""

def plot_sizes(image_sizes, max_n=3000):
    sizes = parallel(image_size, image_files[:max_n], progress=progress_bar)
    
    widths, heights = zip(*sizes)
    min_x = min(widths + heights)
    max_x = max(widths + heights)
    plt.hist(widths, label='width', range=(min_x, max_x))
    plt.hist(heights, label='height', range=(min_x, max_x))
    plt.legend();

image_files = get_image_files(imgs_dir)

plot_sizes(image_files)

"""In the plot above we can see that majority of images are about `3000x2000`. For our first model we would like to try something around `224`. Opening these big images and downsizing by this much will bottleneck the training pipeline (you will notice the GPU will be at 0% usage for long periods).

Let's resize all images to 480 and save them to disk, fastai already provides a handy function for doing this.
"""

if not Path('imgs_480max').exists():
    resize_images(imgs_dir, dest='imgs_480max', max_size=480, progress=progress_bar)

image_files = get_image_files('imgs_480max')
plot_sizes(image_files)

"""## Data Pipeline

Let's not get into the details here as it was already explained in the [previous](https://www.kaggle.com/code/lucasgvazquez/road-to-the-top-part-1-a-dive-to-the-bottom) kernel.
"""

targs_df = pd.read_csv(data_dir/'train.csv')
targs_df = targs_df[targs_df['Image']!='w_7489.jpg']
items = targs_df['Image']

def get_image_path(name):
    return Path('imgs_480max')/name

def get_label(name):
    return targs_df[targs_df['Image']==name]['whaleID'].item()

x_pipe = [get_image_path, PILImage.create]
y_pipe = [get_label, Categorize()]

"""**Extra step:** We won't pay much attention to class balance at this point, however, we do need to make sure we have at least 1 sample per class in the training set. In most cases we don't have to worry about that, we would have plenty of examples per class and a random split would almost certainly contain all classes. But this dataset contains a bunch of classes with a single sample, so we have to manually modify the splits to make sure these are on the training set."""

# make sure at least one of each whale is in training set, then randomly split
must_train_whales = targs_df.groupby('whaleID').first()['Image']
# some magic from [here](https://stackoverflow.com/questions/49823963/get-index-of-one-series-into-another-in-pandas)
must_train_ids = pd.Series(targs_df['Image'].index, index=targs_df['Image']).get(must_train_whales)
must_train_ids = set(must_train_ids)

train_ids, valid_ids = RandomSplitter(seed=42)(items)
print(f"Before: train_ids={len(train_ids)}, valid_ids={len(valid_ids)}")
train_ids = L(set(train_ids).union(must_train_ids))
valid_ids = L(set(valid_ids) - must_train_ids)
print(f"After: train_ids={len(train_ids)}, valid_ids={len(valid_ids)}")
splits = (train_ids, valid_ids)

dss = Datasets(items, [x_pipe, y_pipe], splits=splits)

dss.show(dss[76])

after_item = [ToTensor(), Resize((320, 480))]
after_batch = [IntToFloatTensor(), *aug_transforms(size=(224, 336))]

dls = dss.dataloaders(32, after_item=after_item, after_batch=after_batch)

# Commented out IPython magic to ensure Python compatibility.
# %pip list

dls.show_batch()

"""## Training

No new magic happening here, again refer to the previous blog post for a detailed explanation.
"""

metrics = [error_rate]

learn = vision_learner(dls, 'resnet26d', metrics=metrics).to_fp16()
torch.save(learn.state_dict(), 'my_model.pth')

learn.lr_find()

learn.fine_tune(10, 0.002)

learn.recorder.plot_loss()

"""## Inference

Same function as before, just copy-pasta.
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import cv2

def compute_saliency_map(learn, x):
    learn.model.eval()
    x.requires_grad_()
    
    scores = learn.model(x.unsqueeze(0))
    score_max_index = scores.argmax()
    score_max = scores[0, score_max_index]
    
    score_max.backward()
    saliency_map = x.grad.data.abs().squeeze().cpu().numpy()
    saliency_map = np.max(saliency_map, axis=0)
    
    return saliency_map

def normalize_saliency_map(saliency_map):
    return (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())

def visualize_saliency_map(image, saliency_map):
    saliency_map_normalized = normalize_saliency_map(saliency_map)
    saliency_map_resized = cv2.resize(saliency_map_normalized, (image.width, image.height))
    saliency_map_colored = cv2.applyColorMap(np.uint8(255 * saliency_map_resized), cv2.COLORMAP_JET)
    
    blended = cv2.addWeighted(np.array(image), 0.5, saliency_map_colored, 0.5, 0)
    plt.imshow(blended)
    plt.axis('off')
    plt.show()

dls = dls.cuda() 

# Choose an image from the dataset
img_idx = 38
image = dls.train_ds[img_idx][0]

# Prepare the input tensor
x = dls.after_item(image)
x = x.to('cuda')  # Move the tensor to GPU before calling after_batch
x = dls.after_batch(x.unsqueeze(0))[0]

# Compute and visualize the saliency map
saliency_map = compute_saliency_map(learn, x)
visualize_saliency_map(image, saliency_map)

def submit(learn):
    test_df = pd.read_csv(data_dir / 'sample_submission.csv')
    test_dl = learn.dls.test_dl(test_df['Image'])
    
    preds, targs = learn.get_preds(dl=test_dl)
    
    df = pd.DataFrame(preds.numpy(), columns=learn.dls.vocab)
    df["Image"] = test_df['Image']
    
    preds_path = "submission.csv"
    df.to_csv(preds_path, index=False)
    
    if not iskaggle:
        from kaggle import api
        api.competition_submit_cli(preds_path, "initial submission", comp)

submit(learn)

"""Now we get a score of 4.54! A massive improvement compared to our last score of `30.29923`, and finally better than random guessing at ` 6.10255`!

Indeed this notebook should have been the first one in the series. It was much easier to produce than the last one and provides a decent baseline to start improving.

For our next steps, we can dive deeper into what transforms we want to use, scale up the size of images by using a technique known as progressive resizing and only then start thinking of going bigger with models.
"""